{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a94390a-eb9b-4be2-a116-eca38d9e5ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272115\n",
      "17535\n",
      "20466\n",
      "310116\n"
     ]
    }
   ],
   "source": [
    "#read triples from training set\n",
    "training_f = 'kbc/src_data/FB237/original/train'\n",
    "train_triples = []\n",
    "with open(training_f, 'r') as f:\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if line:\n",
    "            # two relations split by ',', confidence split by tab\n",
    "            line.replace('\\n', '')\n",
    "            lhs = (line.split('\\t')[0])\n",
    "            rel = (line.split('\\t')[1])\n",
    "            rhs = (line.split('\\t')[2])\n",
    "            \n",
    "#             if rel in rel_head_idx:\n",
    "            train_triples.append((lhs, rel, rhs))\n",
    "        else:\n",
    "            break\n",
    "print(len(train_triples))\n",
    "\n",
    "valid_f = 'kbc/src_data/FB237/original/valid'\n",
    "valid_triples = []\n",
    "with open(valid_f, 'r') as f:\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if line:\n",
    "            # two relations split by ',', confidence split by tab\n",
    "            line.replace('\\n', '')\n",
    "            lhs = (line.split('\\t')[0])\n",
    "            rel = (line.split('\\t')[1])\n",
    "            rhs = (line.split('\\t')[2])\n",
    "            \n",
    "#             if rel in rel_head_idx:\n",
    "            valid_triples.append((lhs, rel, rhs))\n",
    "        else:\n",
    "            break\n",
    "print(len(valid_triples))\n",
    "\n",
    "test_f = 'kbc/src_data/FB237/original/test'\n",
    "test_triples = []\n",
    "with open(test_f, 'r') as f:\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if line:\n",
    "            # two relations split by ',', confidence split by tab\n",
    "            line.replace('\\n', '')\n",
    "            lhs = (line.split('\\t')[0])\n",
    "            rel = (line.split('\\t')[1])\n",
    "            rhs = (line.split('\\t')[2])\n",
    "            \n",
    "#             if rel in rel_head_idx:\n",
    "            test_triples.append((lhs, rel, rhs))\n",
    "        else:\n",
    "            break\n",
    "print(len(test_triples))\n",
    "\n",
    "print(len((train_triples + valid_triples + test_triples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cb7a1a9e-3c50-43ec-8924-5f7fdca9ac26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "55\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "# read in all rules (using non-circular rules for now)\n",
    "rule_path = 'kbc/src_data/FB237/original/_entailment_cons.txt'\n",
    "#### read in & convert rule ids and confidence\n",
    "kbc_id_conf_f = rule_path\n",
    "r_p_list = []\n",
    "r_q_list = []\n",
    "conf_list = []\n",
    "neg_list = []\n",
    "r_q_dict = defaultdict(list)\n",
    "with open(kbc_id_conf_f, 'r') as f:\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if line:\n",
    "            # two relations split by ',', confidence split by tab\n",
    "            line.replace('\\n', '')\n",
    "            if '-' in line:\n",
    "                neg_list.append(-1)\n",
    "                line = line[1:]\n",
    "            else:\n",
    "                neg_list.append(1)\n",
    "            r_p = line.split(',')[0]\n",
    "            r_q = line.split(',')[1].split('\\t')[0]\n",
    "            conf = line.split('\\t')[1]\n",
    "            # print(rel0, rel1, conf)\n",
    "            r_p_list.append(r_p)\n",
    "            r_q_list.append(r_q)\n",
    "            conf_list.append(conf)\n",
    "            r_q_dict[r_q].append(r_p)\n",
    "        else:\n",
    "            break\n",
    "rule_list = [r_p_list, r_q_list, conf_list, neg_list]\n",
    "r_set = set(r_p_list + r_q_list)\n",
    "print(len(rule_list[0]))\n",
    "print(len(r_set))\n",
    "print(len(set(r_p_list) - set(r_q_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d598f3-7b02-4c66-b8df-79d8ebbc9863",
   "metadata": {},
   "source": [
    "# Transductive experiments:\n",
    "## Keep only some triplets of all head relations:\n",
    "1. For all rp, in training set, keep only one (e1, rp, e2) for all (e1, rp, _)\n",
    "2. For rp â†’ rq, if (e1, rp, e2), Remove: (e1, rq, _) and (_, rq, e2), (e2, rq, e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "626d5549-b6ee-4cad-a343-710eae36ec04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272115\n",
      "272115\n",
      "264964\n"
     ]
    }
   ],
   "source": [
    "tmp_triples = train_triples.copy()\n",
    "print(len(train_triples))\n",
    "print(len(tmp_triples))\n",
    "seen_dict = {}\n",
    "cnt = 0\n",
    "for e1, r, e2 in train_triples:\n",
    "    for (r_p, r_q) in zip(r_p_list, r_q_list):\n",
    "        if r == r_p and r_p != r_q:\n",
    "            if (e1, r) in seen_dict:\n",
    "                try:\n",
    "                    tmp_triples.remove((e1, r, e2))\n",
    "                    break\n",
    "                except: \n",
    "                    pass\n",
    "            else:\n",
    "                seen_dict[(e1, r)] = (e1, r, e2)\n",
    "                break\n",
    "print(len(tmp_triples))\n",
    "train_triples = tmp_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fff266-c10c-40b7-be81-d838c9e6dff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_triples)\n",
    "new_train_file = open('kbc/src_data/FB237/train_1004', \"w\")\n",
    "for e1, r, e2 in train_triples:\n",
    "    new_train_file.write(str(e1) + '\\t' + str(r) + '\\t' + str(e2))\n",
    "new_train_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d54a64-7461-4878-af05-6e0ed6784941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extraction done\n",
      "261149\n",
      "4547\n",
      "17500\n"
     ]
    }
   ],
   "source": [
    "# filter out some triples with head relations\n",
    "entailment_triples = set()\n",
    "remove_triples = []\n",
    "test_triples = set()\n",
    "head_set = set()\n",
    "tail_set = set()\n",
    "cnt = 0\n",
    "# for e1, r, e2 in train_triples:\n",
    "#     if cnt % 5000 == 0:\n",
    "#         print(cnt)\n",
    "#     cnt += 1\n",
    "#     for (r_p, r_q) in zip(r_p_list, r_q_list):\n",
    "#         if r == r_p and r_p != r_q:\n",
    "#             test_triples.append((e1, r_q, e2))\n",
    "# #             print(e1, r, e2, r_p, r_q)\n",
    "#             tmp = [i for i in train_triples if (i[0] == e1 and i[1] == r_q and i[2] != e2) or (i[0] != e1 and i[1] == r_q and i[2] == e2) or (i[0] == e2 and i[1] == r_q and i[2] != e1) or (i[0] != e2 and i[1] == r_q and i[2] == e1)]\n",
    "#             if len(tmp): \n",
    "# #                 print((tmp[0]))\n",
    "#                 remove_triples += tmp\n",
    "    \n",
    "# remove_triples = list(set(remove_triples))\n",
    "# print(\"extraction done\")\n",
    "# entailment_triples = [i for i in train_triples if i not in remove_triples]\n",
    "# print(len(entailment_triples))\n",
    "# print(len(test_triples))\n",
    "\n",
    "for e1, r, e2 in train_triples:\n",
    "    dup = 0\n",
    "    for (r_p, r_q) in zip(r_p_list, r_q_list):\n",
    "        if r == r_p and r_p != r_q:\n",
    "            test_triples.add((e1, r_q, e2))\n",
    "            if ((e1, r) in head_set) or ((r, e2) in tail_set):\n",
    "                # duplicate, skip\n",
    "                dup = 1\n",
    "                break\n",
    "            else:\n",
    "                entailment_triples.add((e1, r, e2))\n",
    "                head_set.add((e1, r))\n",
    "                tail_set.add((r, e2))\n",
    "    if not dup:\n",
    "        entailment_triples.add((e1, r, e2))\n",
    "    \n",
    "# remove_triples = list(set(remove_triples))\n",
    "print(\"extraction done\")\n",
    "# entailment_triples = [i for i in train_triples if i not in remove_triples]\n",
    "entailment_triples = list(entailment_triples)\n",
    "print(len(entailment_triples))\n",
    "print(len(test_triples))\n",
    "\n",
    "# filter out some triples with head relations from valid set\n",
    "entailment_valid_triples = set()\n",
    "remove_valid_triples = []\n",
    "head_set = set()\n",
    "tail_set = set()\n",
    "cnt = 0\n",
    "for e1, r, e2 in valid_triples:\n",
    "    dup = 0\n",
    "    for (r_p, r_q) in zip(r_p_list, r_q_list):\n",
    "        if r == r_p and r_p != r_q:\n",
    "            if ((e1, r) in head_set) or ((r, e2) in tail_set):\n",
    "                # duplicate, skip\n",
    "                dup = 1\n",
    "                break\n",
    "            else:\n",
    "                head_set.add((e1, r))\n",
    "                tail_set.add((r, e2))\n",
    "                entailment_valid_triples.add((e1, r, e2))\n",
    "    if not dup:\n",
    "        entailment_valid_triples.add((e1, r, e2))\n",
    "\n",
    "entailment_valid_triples = list(entailment_valid_triples)\n",
    "print(len(entailment_valid_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae102b0e-55f8-438f-b8cb-8f0b0607aa93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4547"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(remove_triples)\n",
    "test_triples = list(set(test_triples))\n",
    "len(test_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09d17381-4884-4753-90be-15201ba7b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save triples to file\n",
    "new_train_file = open('kbc/src_data/FB237/train_1004', \"w\")\n",
    "for e1, r, e2 in entailment_triples:\n",
    "    new_train_file.write(str(e1) + '\\t' + str(r) + '\\t' + str(e2))\n",
    "new_train_file.close()\n",
    "\n",
    "new_valid_file = open('kbc/src_data/FB237/valid_1004', \"w\")\n",
    "for e1, r, e2 in tmp_valid_triples:\n",
    "    new_valid_file.write(str(e1) + '\\t' + str(r) + '\\t' + str(e2))\n",
    "new_valid_file.close()\n",
    "\n",
    "new_test_file = open('kbc/src_data/FB237/test_1004', \"w\")\n",
    "for e1, r, e2 in test_triples:\n",
    "    new_test_file.write(str(e1) + '\\t' + str(r) + '\\t' + str(e2))\n",
    "new_test_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829f5e63-3205-4dab-9322-784ed2cccb63",
   "metadata": {},
   "source": [
    "# Inductive experiments:\n",
    "Randomly select from test set, omit any tuples from the training file with the entity in the new test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4993c785-ff18-41b0-a16f-609dfa17fee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20466\n"
     ]
    }
   ],
   "source": [
    "#read triples from test set\n",
    "test_f = 'kbc/src_data/FB237/original/test'\n",
    "test_triples = []\n",
    "with open(test_f, 'r') as f:\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if line:\n",
    "            # two relations split by ',', confidence split by tab\n",
    "            line.replace('\\n', '')\n",
    "            lhs = (line.split('\\t')[0])\n",
    "            rel = (line.split('\\t')[1])\n",
    "            rhs = (line.split('\\t')[2])\n",
    "            \n",
    "            test_triples.append((lhs, rel, rhs))\n",
    "        else:\n",
    "            break\n",
    "print(len(test_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "202b6489-bb5c-4b60-a785-7b8c5cb960b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4093\n",
      "5256\n"
     ]
    }
   ],
   "source": [
    "# randomly select 30% of triples, extract entities\n",
    "import random\n",
    "sample_num = len(test_triples) * 20 // 100\n",
    "sample_test_triples = random.sample(test_triples, sample_num)\n",
    "print(len(sample_test_triples))\n",
    "ent_set = set()\n",
    "for triple in sample_test_triples:\n",
    "    ent_set.add(triple[0])\n",
    "    ent_set.add(triple[2])\n",
    "print(len(ent_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d777aae-ff82-4836-b84f-667bd52a2f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79778\n",
      "3549\n"
     ]
    }
   ],
   "source": [
    "# omit entities from training and valid set\n",
    "tmp_triples = train_triples.copy()\n",
    "for triple in train_triples:\n",
    "    if triple[0] in ent_set or triple[2] in ent_set:\n",
    "        tmp_triples.remove(triple)\n",
    "print(len(tmp_triples))\n",
    "# train_triples = tmp_triples\n",
    "\n",
    "tmp_valid_triples = valid_triples.copy()\n",
    "for triple in valid_triples:\n",
    "    if triple[0] in ent_set or triple[2] in ent_set:\n",
    "        tmp_valid_triples.remove(triple)\n",
    "print(len(tmp_valid_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab935746-7067-4192-8332-dee37b9efd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save triples to file\n",
    "new_train_file = open('kbc/src_data/FB237/train_1007', \"w\")\n",
    "for e1, r, e2 in tmp_triples:\n",
    "    new_train_file.write(str(e1) + '\\t' + str(r) + '\\t' + str(e2))\n",
    "new_train_file.close()\n",
    "\n",
    "new_valid_file = open('kbc/src_data/FB237/valid_1007', \"w\")\n",
    "for e1, r, e2 in tmp_valid_triples:\n",
    "    new_valid_file.write(str(e1) + '\\t' + str(r) + '\\t' + str(e2))\n",
    "new_valid_file.close()\n",
    "\n",
    "new_test_file = open('kbc/src_data/FB237/test_1007', \"w\")\n",
    "for e1, r, e2 in sample_test_triples:\n",
    "    new_test_file.write(str(e1) + '\\t' + str(r) + '\\t' + str(e2))\n",
    "new_test_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a937787-d6e7-4b08-85a3-50b23fe655ae",
   "metadata": {},
   "source": [
    "# Synthesized experiments(10.14):\n",
    "- For (e1, r, e2) in test and r in r_q set, we don't want (e1, r, _), (_, r, e2) to be seen in training and valid set, but we still want e1, r, e2, (e1, r_p, e2) to be seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac099fd7-67ca-49fd-9ac6-41b121da42df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "719\n"
     ]
    }
   ],
   "source": [
    "# remove unrelevant test triples\n",
    "new_test_triples = []\n",
    "r_q_set = set(r_q_list)\n",
    "print(len(r_q_set))\n",
    "for e1, r, e2 in test_triples:\n",
    "    if r in r_q_set:\n",
    "        new_test_triples.append((e1, r, e2))\n",
    "print(len(new_test_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06b2ba7e-e058-4d7c-8329-8646597d59d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650\n",
      "635\n",
      "252430\n",
      "17336\n",
      "253220\n",
      "18103\n"
     ]
    }
   ],
   "source": [
    "head_set = set()\n",
    "tail_set = set()\n",
    "extract_train_triples = set()\n",
    "extract_valid_triples = set()\n",
    "for e1, r, e2 in test_triples:\n",
    "    for (r_p, r_q) in zip(r_p_list, r_q_list):\n",
    "        if r == r_q:\n",
    "            head_set.add((e1, r))\n",
    "#             head_set.add((e1, r_p))\n",
    "            tail_set.add((r, e2))\n",
    "#             tail_set.add((r_p, e2))\n",
    "print(len(head_set))\n",
    "print(len(tail_set))\n",
    "for e1, r, e2 in train_triples:\n",
    "    if (e1, r) not in head_set and (r, e2) not in tail_set:\n",
    "        extract_train_triples.add((e1, r, e2))\n",
    "print(len(extract_train_triples))\n",
    "\n",
    "for e1, r, e2 in valid_triples:\n",
    "    if (e1, r) not in head_set and (r, e2) not in tail_set:\n",
    "        extract_valid_triples.add((e1, r, e2))\n",
    "print(len(extract_valid_triples))\n",
    "\n",
    "# add tail triples to training and valid sets\n",
    "for e1, r, e2 in new_test_triples:\n",
    "    for r_p in r_q_dict[r]:\n",
    "        extract_valid_triples.add((e1, r_p, e2))\n",
    "        extract_triples.add((e1, r_p, e2))\n",
    "print(len(extract_triples))\n",
    "print(len(extract_valid_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48c9cf5a-1df2-437a-a54c-1c3f0bd07091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save triples to file\n",
    "new_train_file = open('kbc/src_data/FB237/train_1014', \"w\")\n",
    "for e1, r, e2 in extract_train_triples:\n",
    "    new_train_file.write(str(e1) + '\\t' + str(r) + '\\t' + str(e2))\n",
    "new_train_file.close()\n",
    "\n",
    "new_valid_file = open('kbc/src_data/FB237/valid_1014', \"w\")\n",
    "for e1, r, e2 in extract_valid_triples:\n",
    "    new_valid_file.write(str(e1) + '\\t' + str(r) + '\\t' + str(e2))\n",
    "new_valid_file.close()\n",
    "\n",
    "new_test_file = open('kbc/src_data/FB237/test_1014', \"w\")\n",
    "for e1, r, e2 in new_test_triples:\n",
    "    new_test_file.write(str(e1) + '\\t' + str(r) + '\\t' + str(e2))\n",
    "new_test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "721fed16-bae2-4f2b-970b-08d9a7f43aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20466\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "print(len(test_triples))\n",
    "print(len(r_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0b3a8083-94f2-4faa-ad46-222c2a414d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_file = open('kbc/src_data/FB237/test_1014', \"w\")\n",
    "for e1, r, e2 in new_test_triples:\n",
    "    new_test_file.write(str(e1) + '\\t' + str(r) + '\\t' + str(e2))\n",
    "new_test_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff729c7-ee8e-4d38-82f1-dfb6178fd57b",
   "metadata": {},
   "source": [
    "# Synthesized experiments(11.14):\n",
    "- Create \"low_freq relation/entity\" test set\n",
    "    - for all r_q, extract all triples from training & test set\n",
    "    - add all such triples to test set (remove redundent), keep only 1 (is this enough?) r_q triple in training set (let the model learn r_q, low_freq, solve the lack of test triple problem)\n",
    "    - only have **8 rules** (r_q not in r_p set)\n",
    "- Validate effect of r_q freq in training set (12.7)\n",
    "    - add all such triples to test set (remove redundent), sample different numbers (1, 2, 4, 8) of r_q triples into training set (let the model learn r_q, low_freq, solve the lack of test triple problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27c9e94a-8d06-46ac-bcf8-f46a4baedb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "16\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "# read in all rules (using rules that r_q are not as r_p as well)\n",
    "rule_path = 'kbc/src_data/FB237/original/_entailment_cons-rq.txt'\n",
    "# rule_path = 'kbc/src_data/FB237/AnyBurl_cons-entailment-target.txt'\n",
    "#### read in & convert rule ids and confidence\n",
    "kbc_id_conf_f = rule_path\n",
    "r_p_list = []\n",
    "r_q_list = []\n",
    "conf_list = []\n",
    "neg_list = []\n",
    "r_q_dict = defaultdict(list)\n",
    "with open(kbc_id_conf_f, 'r') as f:\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if line:\n",
    "            # two relations split by ',', confidence split by tab\n",
    "            line.replace('\\n', '')\n",
    "            if '-' in line:\n",
    "                neg_list.append(-1)\n",
    "                line = line[1:]\n",
    "            else:\n",
    "                neg_list.append(1)\n",
    "            r_p = line.split(',')[0]\n",
    "            r_q = line.split(',')[1].split('\\t')[0]\n",
    "            conf = line.split('\\t')[1]\n",
    "            # print(rel0, rel1, conf)\n",
    "            r_p_list.append(r_p)\n",
    "            r_q_list.append(r_q)\n",
    "            conf_list.append(conf)\n",
    "            r_q_dict[r_q].append(r_p)\n",
    "        else:\n",
    "            break\n",
    "rule_list = [r_p_list, r_q_list, conf_list, neg_list]\n",
    "r_set = set(r_p_list + r_q_list)\n",
    "print(len(rule_list[0]))\n",
    "print(len(r_set))\n",
    "print(len(set(r_p_list) - set(r_q_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e05dd083-8336-4f97-9afe-f951bfc0e129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2245\n",
      "8\n",
      "2253\n"
     ]
    }
   ],
   "source": [
    "# extract all relevant triples in original train, valid and test data\n",
    "# keep sample_num for each relation at training and valid\n",
    "r_q_set = set(r_q_list) - set(r_p_list)\n",
    "# print(r_q_set)\n",
    "r_dict = defaultdict(list)\n",
    "for e1, r, e2 in train_triples + test_triples + valid_triples:\n",
    "    if r in r_q_set:\n",
    "        r_dict[r].append((e1, r, e2))\n",
    "# sample for each relation r_q\n",
    "from random import sample\n",
    "train_num = 1\n",
    "test_num = 80\n",
    "extracted_test_triples = []\n",
    "cand_triples = []\n",
    "all_r_q_triples = set()\n",
    "for r in r_dict:\n",
    "    all_r_q_triples |= set(r_dict[r])\n",
    "    train_r = sample(r_dict[r], train_num)\n",
    "    cand_triples += train_r\n",
    "    test_r = list(set(r_dict[r]) - set(train_r))\n",
    "    extracted_test_triples += test_r\n",
    "print(len(extracted_test_triples))\n",
    "print(len(cand_triples))\n",
    "print(len(all_r_q_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c1955d95-062b-4b98-9bc9-9d2ba2fbf028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269915\n",
      "17521\n"
     ]
    }
   ],
   "source": [
    "# all r_q relevant triples should be removed from original training and testing\n",
    "extract_train_triples = set()\n",
    "extract_valid_triples = set()\n",
    "for t in train_triples:\n",
    "    if t not in all_r_q_triples:\n",
    "        extract_train_triples.add(t)\n",
    "extract_train_triples = extract_train_triples | set(cand_triples)\n",
    "print(len(extract_train_triples))\n",
    "\n",
    "for t in valid_triples:\n",
    "    if t not in all_r_q_triples:\n",
    "        extract_valid_triples.add(t)\n",
    "extract_valid_triples |= set(cand_triples)\n",
    "print(len(extract_valid_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "86856478-8cab-491d-8b9e-67c5a0f106be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2189\n"
     ]
    }
   ],
   "source": [
    "# use for all sampling tests\n",
    "static_extracted_test_triples = list(extracted_test_triples)\n",
    "print(len(static_extracted_test_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d34a49d8-cc0e-4ae7-ab68-95c2c7c72d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save triples to file\n",
    "new_train_file = open('kbc/src_data/FB237/train_1207_1', \"w\")\n",
    "for e1, r, e2 in extract_train_triples:\n",
    "    new_train_file.write(str(e1) + '\\t' + str(r) + '\\t' + str(e2))\n",
    "new_train_file.close()\n",
    "\n",
    "new_valid_file = open('kbc/src_data/FB237/valid_1207_1', \"w\")\n",
    "for e1, r, e2 in extract_valid_triples:\n",
    "    new_valid_file.write(str(e1) + '\\t' + str(r) + '\\t' + str(e2))\n",
    "new_valid_file.close()\n",
    "\n",
    "new_test_file = open('kbc/src_data/FB237/test_1207', \"w\")\n",
    "for e1, r, e2 in static_extracted_test_triples:\n",
    "    new_test_file.write(str(e1) + '\\t' + str(r) + '\\t' + str(e2))\n",
    "new_test_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e355a149-3de7-41e5-859e-96cf6c9007bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Validation experiment(12.7):\n",
    "- Create \"validation\" test set to check the correctness of model on low freq training setting (synthesized setting 11.7)\n",
    "    - remove all r_q relevant triples from training & valid set\n",
    "    - for all <e1, rp, e2> in training set, add <e1, rq, e2> into test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "56963a3f-06fe-47bb-8187-1b72942b399c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10224\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "valid_test_triples = set()\n",
    "r_p_set = set(r_p_list)\n",
    "r_p_dict = collections.defaultdict(list)\n",
    "for r_p, r_q in zip(r_p_list, r_q_list):\n",
    "    r_p_dict[r_p].append(r_q)\n",
    "# print(r_p_dict)\n",
    "for e1, r, e2 in (extract_train_triples | extract_valid_triples):\n",
    "    if r in r_p_dict:\n",
    "        for r_q in r_p_dict[r]:\n",
    "            valid_test_triples.add((e1, r_q, e2))\n",
    "print(len(valid_test_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8308ad82-660c-4ed0-9531-3f404399bdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save triples to file\n",
    "new_train_file = open('kbc/src_data/FB237/train_eval_1207', \"w\")\n",
    "for e1, r, e2 in extract_train_triples:\n",
    "    new_train_file.write(str(e1) + '\\t' + str(r) + '\\t' + str(e2))\n",
    "new_train_file.close()\n",
    "\n",
    "new_valid_file = open('kbc/src_data/FB237/valid_eval_1207', \"w\")\n",
    "for e1, r, e2 in extract_valid_triples:\n",
    "    new_valid_file.write(str(e1) + '\\t' + str(r) + '\\t' + str(e2))\n",
    "new_valid_file.close()\n",
    "\n",
    "new_test_file = open('kbc/src_data/FB237/test_eval_1207', \"w\")\n",
    "for e1, r, e2 in valid_test_triples:\n",
    "    new_test_file.write(str(e1) + '\\t' + str(r) + '\\t' + str(e2))\n",
    "new_test_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dcd65b-404c-4058-97be-52758826d316",
   "metadata": {},
   "source": [
    "# 3.16 type 4 rules synthesized experiments\n",
    "- Create 0-shot experiment for type 4 rules(similar process comparing with 11.7)\n",
    "    - remove all r_q relevant triples from training & valid set\n",
    "    - for all <e1, r1, e2>, <e2, r2, e3> in training set, add <e1, rq, e3> into test set\n",
    "- 4.4 data is generaed using AnyBurl rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46c0f3d4-2aa3-47e6-8914-002c969c8106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule found: /film/film/language,/film/film/country,/location/country/official_language\n",
      "rule found: /people/ethnicity/languages_spoken,/people/ethnicity/people,/people/person/languages\n",
      "rule found: /location/location/contains,/location/location/partially_contains,/location/location/partially_contains\n"
     ]
    }
   ],
   "source": [
    "#### read other types rules from xlsx file\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "## read in each rule, translate, extract triples from all training triples\n",
    "## format: p, q, r, conf: triple_ids\n",
    "dataset = 'FB237'\n",
    "DATA_PATH = Path('/blue/daisyw/ma.haodi/ComplEx-Inject/kbc/data/')\n",
    "path = '/blue/daisyw/ma.haodi/ComplEx-Inject/kbc/src_data/' + dataset\n",
    "rel2id = {}\n",
    "with open(str(DATA_PATH) + '/' + dataset+'/rel_id') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        rel = line.split('\\t')[0]\n",
    "        rel_id = int(line.split('\\t')[1])\n",
    "        rel2id[rel] = rel_id\n",
    "## read in rule set\n",
    "rule_df = pd.read_excel(path+'/original/Freebase_Rules.xlsx', sheet_name=None)['Type 4']\n",
    "# print(rule_df.head())\n",
    "with open(path+'/all_cons_4.txt','w') as out:\n",
    "    for id, row in rule_df.iterrows():\n",
    "        rel_p = '/' + row['p(x,y) <-'].replace('.', '/')\n",
    "        rel_q = '/' + row['q(x,z)'].replace('.', '/')\n",
    "        rel_r = '/' + row['r(z,y)'].replace('.', '/')\n",
    "        conf = row['Confidence']\n",
    "        if conf >= 0.1:\n",
    "            try:\n",
    "                rule = str(rel_p)+','+str(rel_q)+','+str(rel_r)\n",
    "                # check if r_q, r_r exists in dataset\n",
    "                if rel_p in rel2id and rel_q in rel2id and rel_r in rel2id and (rel_r != rel_p and rel_q != rel_p):\n",
    "                    out.write('%s\\t%s\\n' % (rule, conf))\n",
    "                    print(\"rule found: \" + str(rule))\n",
    "                # out2.write(line)\n",
    "            except KeyError:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ba42eb9-956b-447a-9238-4bb053dcffde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule found: /film/film/production_companies\n",
      "rule found: /award/award_category/winners./award/award_honor/ceremony\n",
      "rule found: /film/film/production_companies\n",
      "rule found: /tv/tv_producer/programs_produced./tv/tv_producer_term/program\n",
      "rule found: /film/actor/film./film/performance/special_performance_type\n",
      "rule found: /people/person/place_of_birth\n",
      "rule found: /location/country/second_level_divisions\n",
      "rule found: /film/film/edited_by\n",
      "rule found: /location/statistical_region/places_exported_to./location/imports_and_exports/exported_to\n",
      "rule found: /government/politician/government_positions_held./government/government_position_held/jurisdiction_of_office\n",
      "rule found: /people/person/employment_history./business/employment_tenure/company\n",
      "[['/film/film/country', '/film/film/production_companies', '/base/schemastaging/organization_extra/phone_number./base/schemastaging/phone_sandbox/service_location'], ['/award/award_category/category_of', '/award/award_category/winners./award/award_honor/ceremony', '/time/event/instance_of_recurring_event'], ['/film/film/country', '/film/film/production_companies', '/organization/organization/headquarters./location/mailing_address/country'], ['/award/award_nominee/award_nominations./award/award_nomination/award_nominee', '/tv/tv_producer/programs_produced./tv/tv_producer_term/program', '/film/film/executive_produced_by'], ['/people/person/profession', '/film/actor/film./film/performance/special_performance_type', '/people/profession/specialization_of'], ['/people/person/nationality', '/people/person/place_of_birth', '/location/administrative_division/first_level_division_of'], ['/base/aareas/schema/administrative_area/administrative_parent', '/location/country/second_level_divisions', '/location/administrative_division/country'], ['/film/film/estimated_budget./measurement_unit/dated_money_value/currency', '/film/film/edited_by', '/base/schemastaging/person_extra/net_worth./measurement_unit/dated_money_value/currency'], ['/location/statistical_region/gdp_nominal_per_capita./measurement_unit/dated_money_value/currency', '/location/statistical_region/places_exported_to./location/imports_and_exports/exported_to', '/location/statistical_region/gdp_nominal./measurement_unit/dated_money_value/currency'], ['/people/person/nationality', '/government/politician/government_positions_held./government/government_position_held/jurisdiction_of_office', '/base/biblioness/bibs_location/country'], ['/people/person/nationality', '/people/person/employment_history./business/employment_tenure/company', '/organization/organization/headquarters./location/mailing_address/country']]\n"
     ]
    }
   ],
   "source": [
    "#### AnyBurl Rules \n",
    "#### read other types rules from txt file\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "## read in each rule, translate, extract triples from all training triples\n",
    "## format: p, q, r, conf: triple_ids\n",
    "dataset = 'FB237'\n",
    "DATA_PATH = Path('/blue/daisyw/ma.haodi/ComplEx-Inject/kbc/data/')\n",
    "path = '/blue/daisyw/ma.haodi/ComplEx-Inject/kbc/src_data/' + dataset\n",
    "rel2id = {}\n",
    "with open(str(DATA_PATH) + '/' + dataset+'/rel_id') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        rel = line.split('\\t')[0]\n",
    "        rel_id = int(line.split('\\t')[1])\n",
    "        rel2id[rel] = rel_id\n",
    "\n",
    "# read in rule set\n",
    "rule_list = []\n",
    "with open(os.path.join(path, 'AnyBurl_cons-type_4.txt')) as f: #, open(path+'/all_cons_4.txt','w') as out:\n",
    "    for line in f:\n",
    "        rule, conf = line[:-1].split('\\t')\n",
    "        rel_p, head = rule.split(' <= ')\n",
    "        rel_q, rel_r = head.split(',')\n",
    "        if float(conf) >= 0.8:\n",
    "            try:\n",
    "                # rule = str(rel_p)+','+str(rel_q)+','+str(rel_r)\n",
    "                # out.write('%s\\t%s\\n' % (rule, conf))\n",
    "                rule_list.append([rel_p, rel_q, rel_r])\n",
    "                print(\"rule found: \" + str(rel_q))\n",
    "                # out2.write(line)\n",
    "            except KeyError:\n",
    "                continue\n",
    "print(rule_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1950d662-71c9-420b-91cf-cd9cc4bf740e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['/film/film/country', '/film/film/production_companies', '/base/schemastaging/organization_extra/phone_number./base/schemastaging/phone_sandbox/service_location'], ['/award/award_category/category_of', '/award/award_category/winners./award/award_honor/ceremony', '/time/event/instance_of_recurring_event'], ['/film/film/country', '/film/film/production_companies', '/organization/organization/headquarters./location/mailing_address/country'], ['/award/award_nominee/award_nominations./award/award_nomination/award_nominee', '/tv/tv_producer/programs_produced./tv/tv_producer_term/program', '/film/film/executive_produced_by'], ['/people/person/profession', '/film/actor/film./film/performance/special_performance_type', '/people/profession/specialization_of'], ['/people/person/nationality', '/people/person/place_of_birth', '/location/administrative_division/first_level_division_of'], ['/base/aareas/schema/administrative_area/administrative_parent', '/location/country/second_level_divisions', '/location/administrative_division/country'], ['/film/film/estimated_budget./measurement_unit/dated_money_value/currency', '/film/film/edited_by', '/base/schemastaging/person_extra/net_worth./measurement_unit/dated_money_value/currency'], ['/location/statistical_region/gdp_nominal_per_capita./measurement_unit/dated_money_value/currency', '/location/statistical_region/places_exported_to./location/imports_and_exports/exported_to', '/location/statistical_region/gdp_nominal./measurement_unit/dated_money_value/currency'], ['/people/person/nationality', '/government/politician/government_positions_held./government/government_position_held/jurisdiction_of_office', '/base/biblioness/bibs_location/country'], ['/people/person/nationality', '/people/person/employment_history./business/employment_tenure/company', '/organization/organization/headquarters./location/mailing_address/country']]\n"
     ]
    }
   ],
   "source": [
    "## read in rule set\n",
    "kbc_id_conf_f = path + '/all_cons_4.txt'\n",
    "rule_list = []\n",
    "with open(kbc_id_conf_f, 'r') as f:\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if line:\n",
    "            # format: p, q, r, conf: triple_ids, split by tab\n",
    "            # turn into: p, q, r, conf, tuples (e1, q, e2)\n",
    "            line.replace('\\n', '')\n",
    "            rels = line.split('\\t')[0]\n",
    "            rel_tup = rels.split(',')\n",
    "            rule_list.append(rel_tup)\n",
    "        else:\n",
    "            break\n",
    "print(rule_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecab0592-3e57-474d-b48e-0fdc7bf2abaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "r_p_list = [rule[0] for rule in rule_list]\n",
    "print(len(set((r_p_list))))\n",
    "print(len(rule_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "011ee463-66ed-44d5-9741-36f652812267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39928\n",
      "80\n",
      "40008\n"
     ]
    }
   ],
   "source": [
    "# extract all relevant triples in original train, valid and test data\n",
    "# keep sample_num for each relation at training and valid\n",
    "# create new test data with r_p triples, old test data are replaced\n",
    "# 10-shot test set is used for all tests\n",
    "from collections import defaultdict\n",
    "# print(r_q_set)\n",
    "r_dict = defaultdict(list)\n",
    "for e1, r, e2 in train_triples + test_triples + valid_triples:\n",
    "    if r in r_p_list:\n",
    "        r_dict[r].append((e1, r, e2))\n",
    "# sample for each relation r_q\n",
    "from random import sample\n",
    "train_num = 10\n",
    "test_num = 80\n",
    "extracted_test_triples = []\n",
    "cand_triples = []\n",
    "all_r_p_triples = set()\n",
    "for r in r_dict:\n",
    "    all_r_p_triples |= set(r_dict[r])\n",
    "    train_r = sample(r_dict[r], train_num)\n",
    "    cand_triples += train_r\n",
    "    test_r = list(set(r_dict[r]) - set(train_r))\n",
    "    extracted_test_triples += test_r\n",
    "print(len(extracted_test_triples))\n",
    "print(len(cand_triples))\n",
    "print(len(all_r_p_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "01858a8c-60f9-453c-a164-381169cb0973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "# sample from extract tail triples and add to training, valid\n",
    "# cand_triples are from 10-shot dataset\n",
    "# from collections import defaultdict\n",
    "# r_extract_dict = defaultdict(list)\n",
    "# for e1, r, e2 in cand_triples:\n",
    "#     if r in r_p_list:\n",
    "#         r_dict[r].append((e1, r, e2))\n",
    "# print(r_dict)\n",
    "print(len(cand_triples))\n",
    "# sample for each relation r_p\n",
    "from random import sample\n",
    "train_num = 10\n",
    "sample_r_p_triples = set()\n",
    "for r in r_dict:\n",
    "    train_r = sample(set(r_dict[r]) & set(cand_triples), train_num)\n",
    "    sample_r_p_triples |= set(train_r)\n",
    "print(len(sample_r_p_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ff0fd9b-da37-47e1-8cde-471a8a522679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236530\n",
      "15621\n"
     ]
    }
   ],
   "source": [
    "# all r_p relevant triples should be removed from original training and valid\n",
    "extract_train_triples = set()\n",
    "extract_valid_triples = set()\n",
    "for t in train_triples:\n",
    "    if t not in all_r_p_triples:\n",
    "        extract_train_triples.add(t)\n",
    "extract_train_triples = extract_train_triples | set(sample_r_p_triples)\n",
    "print(len(extract_train_triples))\n",
    "\n",
    "for t in valid_triples:\n",
    "    if t not in all_r_p_triples:\n",
    "        extract_valid_triples.add(t)\n",
    "extract_valid_triples |= set(sample_r_p_triples)\n",
    "print(len(extract_valid_triples))\n",
    "\n",
    "# extract_train_triples = extract_train_triples | set(sample_r_p_triples)\n",
    "# extract_valid_triples |= set(sample_r_p_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2fcfa330-d43e-4d5d-addb-8a3a9ab302fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "#### check overlap\n",
    "print(len(extract_train_triples & extract_valid_triples))\n",
    "print(len(extract_valid_triples & all_r_p_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "298d1f9a-e6c6-402a-9bee-e89221feda39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save triples to file\n",
    "new_train_file = open('kbc/src_data/FB237/few_shot/type_4/train_04_04-10', \"w\")\n",
    "for e1, r, e2 in extract_train_triples:\n",
    "    new_train_file.write(str(e1) + '\\t' + str(r) + '\\t' + str(e2))\n",
    "new_train_file.close()\n",
    "\n",
    "new_valid_file = open('kbc/src_data/FB237/few_shot/type_4/valid_04_04-10', \"w\")\n",
    "for e1, r, e2 in extract_valid_triples:\n",
    "    new_valid_file.write(str(e1) + '\\t' + str(r) + '\\t' + str(e2))\n",
    "new_valid_file.close()\n",
    "\n",
    "# new_test_file = open('kbc/src_data/FB237/few_shot/type_4/test_04_04', \"w\")\n",
    "# for e1, r, e2 in extracted_test_triples:\n",
    "#     new_test_file.write(str(e1) + '\\t' + str(r) + '\\t' + str(e2))\n",
    "# new_test_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce2b6ad-1a3d-4b74-96f5-9919e07aab37",
   "metadata": {},
   "source": [
    "# FB15K:\n",
    "- Rule quality may affect link prediction result\n",
    "    - Entailment rule: try rules with more triples in test/training set\n",
    "    - Type 4: try rules with higher quality (conf > 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e271608b-8fe5-45cb-873a-85877a3396dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
